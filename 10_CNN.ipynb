{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from santander_helper import auc, DataGenerator\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Concatenate, BatchNormalization, Activation, Conv1D, Flatten, Dropout, MaxPool1D, GlobalMaxPool1D\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.regularizers import l1, l2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU, ELU\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from keras_contrib.callbacks import CyclicLR\n",
    "from keras_contrib.layers import PELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.read_csv('train_with_counts.csv').drop(columns=['ID_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.concat([df_train_data, df_train_pseudo], axis=0, sort=False)\n",
    "df_train = df_train_data\n",
    "y = df_train['target'].values\n",
    "df_train_X = df_train.drop(columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for j in range(200):\n",
    "#     df_train_X[f'var_{j}_NEG'] = df_train_X[f'var_{j}']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_vars = False\n",
    "if reverse_vars: \n",
    "    reverse_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 11, 15, 16, 18, 19, 22, 24, 25, 26,\n",
    "                    27, 29, 32, 35, 37, 40, 41, 47, 48, 49, 51, 52, 53, 55, 60, 61,\n",
    "                    62, 65, 66, 67, 69, 70, 71, 74, 78, 79, 82, 84, 89, 90, 91, 94,\n",
    "                    95, 96, 97, 99, 103, 105, 106, 110, 111, 112, 118, 119, 125, 128,\n",
    "                    130, 133, 134, 135, 137, 138, 140, 144, 145, 147, 151, 155, 157,\n",
    "                    159, 161, 162, 163, 164, 167, 168, 170, 171, 173, 175, 176, 179,\n",
    "                    180, 181, 184, 185, 187, 189, 190, 191, 195, 196, 199,\n",
    "                    ]\n",
    "\n",
    "    for j in reverse_list:\n",
    "        df_train_X[f'var_{j}'] *= -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_X_normalized = (df_train_X - df_train_X.mean(axis=0))/df_train_X.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_rows = 2\n",
    "X_train_normalized = np.zeros((df_train_X_normalized.shape[0], common_rows*200, 1))\n",
    "for i in range(200):\n",
    "    X_train_normalized[:, common_rows*i] = df_train_X_normalized[[f'var_{i}']].values\n",
    "    X_train_normalized[:, common_rows*i+1] = df_train_X_normalized[[f'var_{i}_FE']].values\n",
    "    # X_train_normalized[:, common_rows*i+2] = df_train_X_normalized[[f'var_{i}_NEG']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(N_units = 600, kernel_size=common_rows, strides=common_rows):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(N_units, kernel_size=kernel_size, strides=strides, padding='valid', \n",
    "                     # kernel_regularizer=l2(0.01),\n",
    "                     # kernel_regularizer=l1(0.000),\n",
    "                     activation='relu', input_shape=(X_train_normalized.shape[1], 1,)))\n",
    "#     model.add(PELU())\n",
    "#     model.add(LeakyReLU())\n",
    "#     model.add(MaxPool1D(2))\n",
    "    model.add(Flatten())\n",
    "#     model.add(Dropout(0.25))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_file_name = 'best_full_model_aux.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################\n",
      "##################Fold 0#######################\n",
      "###############################################\n",
      "Epoch 1/100\n",
      " - 29s - loss: 0.3210 - auc: 0.8647 - acc: 0.8677 - val_loss: 0.2973 - val_auc: 0.8987 - val_acc: 0.8774\n",
      "\n",
      "Epoch 00001: val_auc improved from -inf to 0.89873, saving model to best_full_model_aux.hdf5\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-31:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/usuario/anaconda3/envs/gpu/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/usuario/anaconda3/envs/gpu/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/usuario/anaconda3/envs/gpu/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 580, in _run\n",
      "    self.sequence.on_epoch_end()\n",
      "  File \"/home/usuario/repos/santander_solved/santander_helper.py\", line 77, in on_epoch_end\n",
      "    common_rows = self.common_rows\n",
      "  File \"/home/usuario/repos/santander_solved/santander_helper.py\", line 43, in augment\n",
      "    xn = np.vstack(xn)\n",
      "  File \"/home/usuario/anaconda3/envs/gpu/lib/python3.6/site-packages/numpy/core/shape_base.py\", line 283, in vstack\n",
      "    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)\n",
      "MemoryError\n",
      "\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "patience = 18\n",
    "epochs = 100\n",
    "bs = 1024\n",
    "N_units = 600\n",
    "class_0_aug = 4\n",
    "class_1_aug = 8\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df_train_X_normalized, y)):\n",
    "    print('###############################################')\n",
    "    print(f'##################Fold {fold}#######################')\n",
    "    print('###############################################')\n",
    "    model = get_model(N_units)\n",
    "    model.compile(Adam(), loss='binary_crossentropy', metrics=[auc, 'accuracy'])\n",
    "    es = EarlyStopping(monitor='val_auc', patience=patience, mode='max', verbose=1)\n",
    "    mc = ModelCheckpoint(best_model_file_name, monitor='val_auc', mode='max', verbose=1, save_best_only=True)\n",
    " \n",
    "    generator = DataGenerator(X_train_normalized[trn_idx], y[trn_idx], \n",
    "                              batch_size=bs, shuffle=True, \n",
    "                              class_1_aug=class_1_aug, \n",
    "                              class_0_aug=class_0_aug,\n",
    "                              common_rows = common_rows\n",
    "                             )\n",
    "    tr_iter_in_epoch = generator.__len__()\n",
    "#     gamma = 1 - 6e-05  * 4*312/tr_iter_in_epoch\n",
    "#     clr = CyclicLR(base_lr=0.0001, max_lr=0.005, step_size=4*tr_iter_in_epoch, mode='exp_range', gamma=gamma)\n",
    "    clr = CyclicLR(base_lr=0.0001, max_lr=0.005, step_size=4*tr_iter_in_epoch, mode='triangular2')\n",
    "    X_val_data, y_val_data = DataGenerator.augment(X_train_normalized[val_idx], \n",
    "                                     y[val_idx], class_1_aug=class_1_aug, class_0_aug=class_0_aug, common_rows = common_rows)\n",
    "    indexes_val = np.arange(len(y_val_data))\n",
    "    np.random.shuffle(indexes_val)\n",
    "    model.fit_generator(generator,\n",
    "              epochs=epochs,\n",
    "              verbose=2,\n",
    "              callbacks = [es, \n",
    "                           mc, \n",
    "                           clr],\n",
    "              # validation_data=(X_train_normalized[val_idx], y[val_idx].reshape(-1,1)*np.ones((len(val_idx), 200))))\n",
    "              validation_data=(X_val_data[indexes_val], y_val_data[indexes_val])\n",
    "              # validation_data=(X_train_normalized[val_idx], y[val_idx])\n",
    "                )\n",
    "    # print(f'Finish training with lr {lr}')\n",
    "    model = get_model()\n",
    "    # Load weights from ModelCheckpoint\n",
    "    model.load_weights(best_model_file_name)\n",
    "    # Save them to disk\n",
    "    model.save_weights(f'models/CNN_generator_fold_{fold}_cl1_{class_1_aug}_cl0_{class_0_aug}_{N_units}_rev.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
